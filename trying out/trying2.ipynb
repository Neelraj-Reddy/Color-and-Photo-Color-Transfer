{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neelraj-reddy/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/neelraj-reddy/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color transfer completed. Output saved as 'output.jpg'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import KDTree\n",
    "import cv2\n",
    "\n",
    "# Load DeepLabV3 model (set to evaluation mode)\n",
    "model = deeplabv3_resnet101(pretrained=True).eval()\n",
    "\n",
    "def get_segmentation_mask(image_path, resize_dim=(1024, 1024)):\n",
    "    \"\"\"\n",
    "    Load an image, resize it to a higher resolution, and obtain the segmentation mask.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    # Resize to higher resolution to improve segmentation\n",
    "    image_resized = image.resize(resize_dim)\n",
    "    \n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = transform(image_resized).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)[\"out\"][0]\n",
    "    # Compute the predicted class for each pixel\n",
    "    mask = output.argmax(0).byte().cpu().numpy()\n",
    "    return image_resized, mask\n",
    "\n",
    "def segment_image(image, mask, target_class=15):\n",
    "    \"\"\"\n",
    "    Segment the image into foreground and background based on a target class.\n",
    "    By default, target_class=15 (commonly representing 'person' in COCO).\n",
    "    \"\"\"\n",
    "    # Create a binary mask for the target class\n",
    "    mask_fg = np.zeros_like(mask)\n",
    "    mask_fg[mask == target_class] = 1\n",
    "\n",
    "    # Multiply the image by the mask to obtain foreground and background images\n",
    "    image_np = np.array(image)\n",
    "    segmented_fg = image_np * np.expand_dims(mask_fg, axis=-1)\n",
    "    segmented_bg = image_np * np.expand_dims(1 - mask_fg, axis=-1)\n",
    "    return segmented_fg, segmented_bg\n",
    "\n",
    "def get_palette(image_array, n_colors=15):\n",
    "    \"\"\"\n",
    "    Extract dominant colors using K-Means clustering.\n",
    "    Increase n_colors to capture more fine details.\n",
    "    \"\"\"\n",
    "    pixels = image_array.reshape(-1, 3)\n",
    "    valid = (pixels.sum(axis=1) > 0)\n",
    "    pixels = pixels[valid]\n",
    "\n",
    "    if len(pixels) == 0:\n",
    "        return np.array([[0, 0, 0]])\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_colors, random_state=42).fit(pixels)\n",
    "    return kmeans.cluster_centers_.astype(int)\n",
    "\n",
    "def apply_palette(target, ref_palette):\n",
    "    \"\"\"\n",
    "    Map target image colors to the nearest colors in the reference palette using float32.\n",
    "    \"\"\"\n",
    "    target_flat = target.reshape(-1, 3).astype(np.float32)\n",
    "    valid_mask = (target_flat.sum(axis=1) > 0)\n",
    "\n",
    "    if np.sum(valid_mask) == 0:\n",
    "        return target\n",
    "\n",
    "    target_valid = target_flat[valid_mask]\n",
    "\n",
    "    # Build KDTree for fast nearest-neighbor lookup with float precision\n",
    "    tree = KDTree(ref_palette.astype(np.float32))\n",
    "    _, nearest_indices = tree.query(target_valid)\n",
    "\n",
    "    # Map the nearest palette color to the target pixels\n",
    "    target_flat[valid_mask] = ref_palette[nearest_indices]\n",
    "    return target_flat.reshape(target.shape).astype(np.uint8)\n",
    "\n",
    "def apply_smoothing(image_np, d=9, sigmaColor=75, sigmaSpace=75):\n",
    "    \"\"\"\n",
    "    Apply bilateral filtering to smooth and preserve edges.\n",
    "    \"\"\"\n",
    "    return cv2.bilateralFilter(image_np.astype(np.uint8), d, sigmaColor, sigmaSpace)\n",
    "\n",
    "def color_transfer_with_segmentation(input_img, ref_img, target_class=15):\n",
    "    \"\"\"\n",
    "    Main function to perform color transfer:\n",
    "      - Segment the input and reference images.\n",
    "      - Extract dominant color palettes for foreground and background.\n",
    "      - Apply the reference palettes to the corresponding regions in the input image.\n",
    "      - Apply smoothing to improve quality.\n",
    "    \"\"\"\n",
    "    # Get segmentation masks (images and masks are resized to 1024x1024)\n",
    "    input_image, input_mask = get_segmentation_mask(input_img)\n",
    "    ref_image, ref_mask = get_segmentation_mask(ref_img)\n",
    "\n",
    "    # Segment images into foreground and background\n",
    "    input_fg, input_bg = segment_image(input_image, input_mask, target_class)\n",
    "    ref_fg, ref_bg = segment_image(ref_image, ref_mask, target_class)\n",
    "\n",
    "    # Extract color palettes from the reference image segments\n",
    "    fg_palette = get_palette(ref_fg, n_colors=15)  # Increased n_colors\n",
    "    bg_palette = get_palette(ref_bg, n_colors=15)\n",
    "\n",
    "    # Apply the color palettes to the input image segments\n",
    "    fg_matched = apply_palette(input_fg, fg_palette)\n",
    "    bg_matched = apply_palette(input_bg, bg_palette)\n",
    "\n",
    "    # Merge the processed foreground and background using the binary mask\n",
    "    mask_fg = (input_mask == target_class)[..., None]\n",
    "    result = np.where(mask_fg, fg_matched, bg_matched)\n",
    "\n",
    "    # Apply bilateral filtering for smoother results\n",
    "    result_smoothed = apply_smoothing(result)\n",
    "\n",
    "    return Image.fromarray(result_smoothed)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Input and reference image paths\n",
    "    input_img_path = \"input.jpg\"\n",
    "    ref_img_path = \"reference.jpeg\"\n",
    "\n",
    "    # Apply color transfer with segmentation\n",
    "    result_image = color_transfer_with_segmentation(input_img_path, ref_img_path)\n",
    "\n",
    "    # Save the result\n",
    "    result_image.save(\"output.jpg\")\n",
    "    print(\"Color transfer completed. Output saved as 'output.jpg'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neelraj-reddy/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/neelraj-reddy/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1473: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (15). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color transfer completed. Output saved as 'output.jpg'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import KDTree\n",
    "import cv2\n",
    "import cv2 as cv\n",
    "\n",
    "# Load DeepLabV3 model (set to evaluation mode)\n",
    "model = deeplabv3_resnet101(pretrained=True).eval()\n",
    "\n",
    "def get_segmentation_mask(image_path, resize_dim=(1024, 1024)):\n",
    "    \"\"\"\n",
    "    Load an image, resize it to a higher resolution, and obtain the segmentation mask.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    # Resize to higher resolution to improve segmentation\n",
    "    image_resized = image.resize(resize_dim)\n",
    "    \n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = transform(image_resized).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)[\"out\"][0]\n",
    "    # Compute the predicted class for each pixel\n",
    "    mask = output.argmax(0).byte().cpu().numpy()\n",
    "    return np.array(image_resized), mask\n",
    "\n",
    "def segment_image(image, mask, target_class=15):\n",
    "    \"\"\"\n",
    "    Segment the image into foreground and background based on a target class.\n",
    "    By default, target_class=15 (commonly representing 'person' in COCO).\n",
    "    \"\"\"\n",
    "    # Create a binary mask for the target class\n",
    "    mask_fg = np.zeros_like(mask)\n",
    "    mask_fg[mask == target_class] = 1\n",
    "\n",
    "    # Multiply the image by the mask to obtain foreground and background images\n",
    "    segmented_fg = image * np.expand_dims(mask_fg, axis=-1)\n",
    "    segmented_bg = image * np.expand_dims(1 - mask_fg, axis=-1)\n",
    "    return segmented_fg, segmented_bg\n",
    "\n",
    "def rgb_to_lab(image):\n",
    "    \"\"\"\n",
    "    Convert an RGB image to Lab color space.\n",
    "    \"\"\"\n",
    "    return cv.cvtColor(image, cv.COLOR_RGB2LAB)\n",
    "\n",
    "def lab_to_rgb(image):\n",
    "    \"\"\"\n",
    "    Convert a Lab image back to RGB.\n",
    "    \"\"\"\n",
    "    return cv.cvtColor(image, cv.COLOR_LAB2RGB)\n",
    "\n",
    "def get_palette(image_array, n_colors=15):\n",
    "    \"\"\"\n",
    "    Extract dominant colors using K-Means clustering.\n",
    "    Increase n_colors to capture more fine details.\n",
    "    \"\"\"\n",
    "    pixels = image_array.reshape(-1, 3)\n",
    "    valid = (pixels.sum(axis=1) > 0)\n",
    "    pixels = pixels[valid]\n",
    "\n",
    "    if len(pixels) == 0:\n",
    "        return np.array([[0, 0, 0]])\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_colors, random_state=42).fit(pixels)\n",
    "    return kmeans.cluster_centers_.astype(np.float32)\n",
    "\n",
    "def apply_palette_lab(target, ref_palette):\n",
    "    \"\"\"\n",
    "    Map target image colors to the nearest colors in the reference palette in Lab space.\n",
    "    \"\"\"\n",
    "    target_flat = target.reshape(-1, 3).astype(np.float32)\n",
    "    valid_mask = (target_flat.sum(axis=1) > 0)\n",
    "\n",
    "    if np.sum(valid_mask) == 0:\n",
    "        return target\n",
    "\n",
    "    target_valid = target_flat[valid_mask]\n",
    "\n",
    "    # Build KDTree for fast nearest-neighbor lookup with float precision\n",
    "    tree = KDTree(ref_palette)\n",
    "    _, nearest_indices = tree.query(target_valid)\n",
    "\n",
    "    # Map the nearest palette color to the target pixels\n",
    "    target_flat[valid_mask] = ref_palette[nearest_indices]\n",
    "    return target_flat.reshape(target.shape).astype(np.uint8)\n",
    "\n",
    "def apply_smoothing(image_np, d=9, sigmaColor=75, sigmaSpace=75):\n",
    "    \"\"\"\n",
    "    Apply bilateral filtering to smooth and preserve edges.\n",
    "    \"\"\"\n",
    "    return cv2.bilateralFilter(image_np.astype(np.uint8), d, sigmaColor, sigmaSpace)\n",
    "\n",
    "def color_transfer_with_segmentation(input_img, ref_img, target_class=15):\n",
    "    \"\"\"\n",
    "    Main function to perform color transfer using Lab color space:\n",
    "      - Segment the input and reference images.\n",
    "      - Extract dominant color palettes for foreground and background in Lab space.\n",
    "      - Apply the reference palettes to the corresponding regions in the input image.\n",
    "      - Apply smoothing to improve quality.\n",
    "    \"\"\"\n",
    "    # Get segmentation masks (images and masks are resized to 1024x1024)\n",
    "    input_image, input_mask = get_segmentation_mask(input_img)\n",
    "    ref_image, ref_mask = get_segmentation_mask(ref_img)\n",
    "\n",
    "    # Segment images into foreground and background\n",
    "    input_fg, input_bg = segment_image(input_image, input_mask, target_class)\n",
    "    ref_fg, ref_bg = segment_image(ref_image, ref_mask, target_class)\n",
    "\n",
    "    # Convert to Lab color space\n",
    "    input_fg_lab = rgb_to_lab(input_fg)\n",
    "    input_bg_lab = rgb_to_lab(input_bg)\n",
    "    ref_fg_lab = rgb_to_lab(ref_fg)\n",
    "    ref_bg_lab = rgb_to_lab(ref_bg)\n",
    "\n",
    "    # Extract color palettes from the reference image segments in Lab space\n",
    "    fg_palette_lab = get_palette(ref_fg_lab, n_colors=15)  # Increased n_colors\n",
    "    bg_palette_lab = get_palette(ref_bg_lab, n_colors=15)\n",
    "\n",
    "    # Apply the color palettes to the input image segments in Lab space\n",
    "    fg_matched_lab = apply_palette_lab(input_fg_lab, fg_palette_lab)\n",
    "    bg_matched_lab = apply_palette_lab(input_bg_lab, bg_palette_lab)\n",
    "\n",
    "    # Merge the processed foreground and background using the binary mask\n",
    "    mask_fg = (input_mask == target_class)[..., None]\n",
    "    result_lab = np.where(mask_fg, fg_matched_lab, bg_matched_lab)\n",
    "\n",
    "    # Apply bilateral filtering for smoother results\n",
    "    result_smoothed = apply_smoothing(result_lab)\n",
    "\n",
    "    # Convert back to RGB\n",
    "    result_rgb = lab_to_rgb(result_smoothed)\n",
    "    return Image.fromarray(result_rgb)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Input and reference image paths\n",
    "    input_img_path = \"input.jpg\"\n",
    "    ref_img_path = \"reference.jpeg\"\n",
    "\n",
    "    # Apply color transfer with segmentation\n",
    "    result_image = color_transfer_with_segmentation(input_img_path, ref_img_path)\n",
    "\n",
    "    # Save the result\n",
    "    result_image.save(\"output.jpg\")\n",
    "    print(\"Color transfer completed. Output saved as 'output.jpg'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neelraj-reddy/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/neelraj-reddy/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1473: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (20). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color transfer completed. Output saved as 'output.jpg'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@707.084] global loadsave.cpp:848 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import KDTree\n",
    "import cv2\n",
    "import cv2 as cv\n",
    "from skimage import exposure\n",
    "\n",
    "\n",
    "# Load DeepLabV3 model (set to evaluation mode)\n",
    "model = deeplabv3_resnet101(pretrained=True).eval()\n",
    "\n",
    "def get_segmentation_mask(image_path, resize_dim=(1024, 1024)):\n",
    "    \"\"\"\n",
    "    Load an image, resize it to a higher resolution, and obtain the segmentation mask.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    # Resize to higher resolution to improve segmentation\n",
    "    image_resized = image.resize(resize_dim)\n",
    "    \n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = transform(image_resized).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)[\"out\"][0]\n",
    "    # Compute the predicted class for each pixel\n",
    "    mask = output.argmax(0).byte().cpu().numpy()\n",
    "    return np.array(image_resized), mask\n",
    "\n",
    "\n",
    "def segment_image(image, mask, target_class=15):\n",
    "    \"\"\"\n",
    "    Segment the image into foreground and background based on a target class.\n",
    "    Default: target_class=15 (commonly representing 'person' in COCO).\n",
    "    \"\"\"\n",
    "    # Create a binary mask for the target class\n",
    "    mask_fg = np.zeros_like(mask)\n",
    "    mask_fg[mask == target_class] = 1\n",
    "\n",
    "    # Multiply the image by the mask to obtain foreground and background images\n",
    "    segmented_fg = image * np.expand_dims(mask_fg, axis=-1)\n",
    "    segmented_bg = image * np.expand_dims(1 - mask_fg, axis=-1)\n",
    "    return segmented_fg, segmented_bg\n",
    "\n",
    "\n",
    "def rgb_to_lab(image):\n",
    "    \"\"\"\n",
    "    Convert an RGB image to Lab color space.\n",
    "    \"\"\"\n",
    "    return cv.cvtColor(image, cv.COLOR_RGB2LAB)\n",
    "\n",
    "\n",
    "def lab_to_rgb(image):\n",
    "    \"\"\"\n",
    "    Convert a Lab image back to RGB.\n",
    "    \"\"\"\n",
    "    return cv.cvtColor(image, cv.COLOR_LAB2RGB)\n",
    "\n",
    "\n",
    "def adaptive_palette_size(image_array, base_colors=10, max_colors=20):\n",
    "    \"\"\"\n",
    "    Dynamically choose the number of clusters based on region complexity.\n",
    "    \"\"\"\n",
    "    pixel_count = np.count_nonzero(image_array.sum(axis=2) > 0)\n",
    "    if pixel_count > 50000:  # More complex region\n",
    "        return max_colors\n",
    "    else:\n",
    "        return base_colors\n",
    "\n",
    "\n",
    "def get_palette(image_array, base_colors=10, max_colors=20):\n",
    "    \"\"\"\n",
    "    Extract dominant colors using K-Means clustering.\n",
    "    Adaptively adjust the number of clusters based on region complexity.\n",
    "    \"\"\"\n",
    "    pixels = image_array.reshape(-1, 3)\n",
    "    valid = (pixels.sum(axis=1) > 0)\n",
    "    pixels = pixels[valid]\n",
    "\n",
    "    if len(pixels) == 0:\n",
    "        return np.array([[0, 0, 0]])\n",
    "\n",
    "    # Dynamically adjust n_colors\n",
    "    n_colors = adaptive_palette_size(image_array, base_colors, max_colors)\n",
    "    kmeans = KMeans(n_clusters=n_colors, random_state=42).fit(pixels)\n",
    "    return kmeans.cluster_centers_.astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_palette_lab(target, ref_palette):\n",
    "    \"\"\"\n",
    "    Map target image colors to the nearest colors in the reference palette in Lab space.\n",
    "    \"\"\"\n",
    "    target_flat = target.reshape(-1, 3).astype(np.float32)\n",
    "    valid_mask = (target_flat.sum(axis=1) > 0)\n",
    "\n",
    "    if np.sum(valid_mask) == 0:\n",
    "        return target\n",
    "\n",
    "    target_valid = target_flat[valid_mask]\n",
    "\n",
    "    # Build KDTree for fast nearest-neighbor lookup with float precision\n",
    "    tree = KDTree(ref_palette)\n",
    "    _, nearest_indices = tree.query(target_valid)\n",
    "\n",
    "    # Map the nearest palette color to the target pixels\n",
    "    target_flat[valid_mask] = ref_palette[nearest_indices]\n",
    "    return target_flat.reshape(target.shape).astype(np.uint8)\n",
    "\n",
    "\n",
    "def apply_histogram_matching_lab(input_lab, ref_lab):\n",
    "    \"\"\"\n",
    "    Match histograms of input and reference in Lab space for better color consistency.\n",
    "    \"\"\"\n",
    "    matched_lab = np.zeros_like(input_lab)\n",
    "    for i in range(3):  # Match histograms for L, a, and b channels\n",
    "        matched_lab[:, :, i] = exposure.match_histograms(\n",
    "            input_lab[:, :, i], ref_lab[:, :, i], channel_axis=None\n",
    "        )\n",
    "    return matched_lab\n",
    "\n",
    "\n",
    "def apply_clahe(image_lab):\n",
    "    \"\"\"\n",
    "    Apply CLAHE on the L channel to enhance contrast.\n",
    "    \"\"\"\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    image_lab[:, :, 0] = clahe.apply(image_lab[:, :, 0])  # Enhance only L channel\n",
    "    return image_lab\n",
    "\n",
    "\n",
    "def blend_edges(original, transferred, mask, alpha=0.8):\n",
    "    \"\"\"\n",
    "    Blend the original and transferred images near edges for smoother results.\n",
    "    \"\"\"\n",
    "    edges = cv2.Canny(mask.astype(np.uint8) * 255, 100, 200)\n",
    "    edges_dilated = cv2.dilate(edges, np.ones((3, 3), np.uint8), iterations=1)\n",
    "\n",
    "    # Create edge mask\n",
    "    edge_mask = cv2.GaussianBlur(edges_dilated, (5, 5), 0) / 255.0\n",
    "    blended = (alpha * transferred + (1 - alpha) * original).astype(np.uint8)\n",
    "    \n",
    "    result = blended * edge_mask[..., None] + transferred * (1 - edge_mask[..., None])\n",
    "    return result\n",
    "\n",
    "\n",
    "def guided_filter(input_img, guidance_img, radius=8, eps=1e-2):\n",
    "    \"\"\"\n",
    "    Apply guided filtering for edge-aware smoothing.\n",
    "    \"\"\"\n",
    "    if input_img.dtype != np.float32:\n",
    "        input_img = input_img.astype(np.float32) / 255.0\n",
    "    if guidance_img.dtype != np.float32:\n",
    "        guidance_img = guidance_img.astype(np.float32) / 255.0\n",
    "\n",
    "    # Apply guided filtering and convert back to 8-bit\n",
    "    filtered_img = cv.ximgproc.guidedFilter(\n",
    "        guide=guidance_img, src=input_img, radius=radius, eps=eps\n",
    "    )\n",
    "    return (filtered_img * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def color_transfer_with_segmentation(input_img, ref_img, target_class=15, debug=False):\n",
    "    \"\"\"\n",
    "    Main function to perform enhanced color transfer:\n",
    "      - Segment the input and reference images.\n",
    "      - Extract dominant color palettes in Lab space.\n",
    "      - Apply histogram matching for fine-tuning.\n",
    "      - Blend transferred results with original edges.\n",
    "      - Apply CLAHE and guided filtering for quality improvement.\n",
    "    \"\"\"\n",
    "    # Get segmentation masks (images and masks are resized to 1024x1024)\n",
    "    input_image, input_mask = get_segmentation_mask(input_img)\n",
    "    ref_image, ref_mask = get_segmentation_mask(ref_img)\n",
    "\n",
    "    # Segment images into foreground and background\n",
    "    input_fg, input_bg = segment_image(input_image, input_mask, target_class)\n",
    "    ref_fg, ref_bg = segment_image(ref_image, ref_mask, target_class)\n",
    "\n",
    "    # Convert to Lab color space\n",
    "    input_fg_lab = rgb_to_lab(input_fg)\n",
    "    input_bg_lab = rgb_to_lab(input_bg)\n",
    "    ref_fg_lab = rgb_to_lab(ref_fg)\n",
    "    ref_bg_lab = rgb_to_lab(ref_bg)\n",
    "\n",
    "    # Extract adaptive color palettes from the reference image segments in Lab space\n",
    "    fg_palette_lab = get_palette(ref_fg_lab, base_colors=10, max_colors=20)\n",
    "    bg_palette_lab = get_palette(ref_bg_lab, base_colors=10, max_colors=20)\n",
    "\n",
    "    # Apply the color palettes to the input image segments in Lab space\n",
    "    fg_matched_lab = apply_palette_lab(input_fg_lab, fg_palette_lab)\n",
    "    bg_matched_lab = apply_palette_lab(input_bg_lab, bg_palette_lab)\n",
    "\n",
    "    # Merge the processed foreground and background using the binary mask\n",
    "    mask_fg = (input_mask == target_class)[..., None]\n",
    "    result_lab = np.where(mask_fg, fg_matched_lab, bg_matched_lab)\n",
    "\n",
    "    # Apply histogram matching for better color consistency\n",
    "    result_matched_lab = apply_histogram_matching_lab(result_lab, ref_fg_lab)\n",
    "\n",
    "    # Apply CLAHE to enhance contrast\n",
    "    result_clahe_lab = apply_clahe(result_matched_lab)\n",
    "\n",
    "    # Convert back to RGB\n",
    "    result_rgb = lab_to_rgb(result_clahe_lab)\n",
    "\n",
    "    # Blend original and transferred images near edges\n",
    "    result_blended = blend_edges(input_image, result_rgb, input_mask)\n",
    "\n",
    "    # Apply guided filtering for smoother and edge-preserving results\n",
    "    result_filtered = guided_filter(result_blended, input_image)\n",
    "\n",
    "    # Save intermediate results for debugging if enabled\n",
    "    if debug:\n",
    "        cv2.imwrite(\"debug_fg_matched.jpg\", lab_to_rgb(fg_matched_lab))\n",
    "        cv2.imwrite(\"debug_bg_matched.jpg\", lab_to_rgb(bg_matched_lab))\n",
    "        cv2.imwrite(\"debug_result_blended.jpg\", result_blended)\n",
    "\n",
    "    return Image.fromarray(result_filtered.astype(np.uint8))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Input and reference image paths\n",
    "    input_img_path = \"input.jpg\"\n",
    "    ref_img_path = \"reference.jpeg\"\n",
    "\n",
    "    # Set target class (15 is for person by default)\n",
    "    target_class = 15\n",
    "\n",
    "    # Apply enhanced color transfer with segmentation\n",
    "    result_image = color_transfer_with_segmentation(input_img_path, ref_img_path, target_class, debug=True)\n",
    "\n",
    "    # Save the result\n",
    "    result_image.save(\"output.jpg\")\n",
    "    print(\"Color transfer completed. Output saved as 'output.jpg'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/neelraj-reddy/college/6th_sem/computer vision/project/trying out/input3.jpg: 448x640 1 dog, 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 2.4ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11n-seg.pt\")  # load an official model\n",
    "model = YOLO(\"/home/neelraj-reddy/college/6th_sem/computer vision/project/trying out/yolo11n-seg.pt\")  # load a custom model\n",
    "\n",
    "# Predict with the model\n",
    "results = model(\"/home/neelraj-reddy/college/6th_sem/computer vision/project/trying out/input3.jpg\")  # predict on an image\n",
    "\n",
    "# Access the results\n",
    "for result in results:\n",
    "    xy = result.masks.xy  # mask in polygon format\n",
    "    xyn = result.masks.xyn  # normalized\n",
    "    masks = result.masks.data  # mask in matrix format (num_objects x H x W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process results list\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.show()  # display to screen\n",
    "    result.save(filename=\"result.jpg\")  # save to disk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
