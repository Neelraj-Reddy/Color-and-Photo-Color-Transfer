{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Load pre-trained Mask R-CNN model\n",
    "def load_mask_rcnn():\n",
    "    model = models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Preprocess input image\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    return image, transform(image_rgb).unsqueeze(0)\n",
    "\n",
    "# Get segmentation mask\n",
    "def get_segmentation_mask(model, image_tensor, threshold=0.5):\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)\n",
    "\n",
    "    masks = predictions[0]['masks']\n",
    "    labels = predictions[0]['labels']\n",
    "    scores = predictions[0]['scores']\n",
    "\n",
    "    # Filter masks based on threshold\n",
    "    mask_list = []\n",
    "    for i in range(len(masks)):\n",
    "        if scores[i] >= threshold and labels[i] == 18:  # 18 is the COCO class for dog\n",
    "            mask_list.append(masks[i, 0].cpu().numpy())\n",
    "\n",
    "    # Combine masks if multiple\n",
    "    if len(mask_list) > 0:\n",
    "        mask = np.sum(mask_list, axis=0)\n",
    "        mask = (mask > 0.5).astype(np.uint8) * 255\n",
    "        return mask\n",
    "    else:\n",
    "        return np.zeros((image_tensor.shape[2], image_tensor.shape[3]), dtype=np.uint8)\n",
    "\n",
    "# Generate foreground and background masks\n",
    "def segment_foreground_background(image, mask):\n",
    "    # Resize mask to match image size\n",
    "    mask_resized = cv2.resize(mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Apply mask to extract foreground and background\n",
    "    fg = cv2.bitwise_and(image, image, mask=mask_resized)\n",
    "    bg_mask = 255 - mask_resized\n",
    "    bg = cv2.bitwise_and(image, image, mask=bg_mask)\n",
    "\n",
    "    return fg, bg\n",
    "\n",
    "# Main function\n",
    "def main(image_path):\n",
    "    # Load Mask R-CNN model\n",
    "    model = load_mask_rcnn()\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image, image_tensor = preprocess_image(image_path)\n",
    "\n",
    "    # Get segmentation mask for the dog\n",
    "    mask = get_segmentation_mask(model, image_tensor)\n",
    "\n",
    "    # Segment foreground and background\n",
    "    fg, bg = segment_foreground_background(image, mask)\n",
    "\n",
    "    # Save segmented images\n",
    "    cv2.imwrite(\"foreground.jpg\", fg)\n",
    "    cv2.imwrite(\"background.jpg\", bg)\n",
    "    print(\"Segmentation complete. Foreground and background saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neelraj-reddy/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /home/neelraj-reddy/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n",
      "100%|██████████| 170M/170M [06:49<00:00, 435kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation complete. Foreground and background saved.\n"
     ]
    }
   ],
   "source": [
    "# Run segmentation on input image\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"/home/neelraj-reddy/college/6th_sem/computer vision/project/A little survey on previous works/images/dog.jpeg\"          # Input source image\n",
    "    main(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# Load SAM model\n",
    "def load_sam_model(model_type=\"vit_b\"):\n",
    "    checkpoint_path = \"sam_vit_b_01ec64.pth\"  # Path to SAM checkpoint\n",
    "    model = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "    model.eval()\n",
    "    predictor = SamPredictor(model)\n",
    "    return predictor\n",
    "\n",
    "# Preprocess image and get SAM mask\n",
    "def get_sam_mask(predictor, image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Set image for SAM predictor\n",
    "    predictor.set_image(image_rgb)\n",
    "\n",
    "    # Get initial automatic mask\n",
    "    masks, _, _ = predictor.predict(\n",
    "        point_coords=None,  # No specific point prompt\n",
    "        point_labels=None,\n",
    "        box=None,\n",
    "        multimask_output=True\n",
    "    )\n",
    "\n",
    "    # Select the best mask based on area (largest mask is often the foreground)\n",
    "    best_mask = max(masks, key=lambda x: np.sum(x))\n",
    "    mask = best_mask.astype(np.uint8) * 255\n",
    "    return image, mask\n",
    "\n",
    "# Segment foreground and background\n",
    "def segment_foreground_background(image, mask):\n",
    "    mask_resized = cv2.resize(mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Apply mask to extract foreground and background\n",
    "    fg = cv2.bitwise_and(image, image, mask=mask_resized)\n",
    "    bg_mask = 255 - mask_resized\n",
    "    bg = cv2.bitwise_and(image, image, mask=bg_mask)\n",
    "\n",
    "    return fg, bg\n",
    "\n",
    "# Save and display results\n",
    "def save_results(fg, bg):\n",
    "    cv2.imwrite(\"foreground.jpg\", fg)\n",
    "    cv2.imwrite(\"background.jpg\", bg)\n",
    "    print(\"Segmentation complete. Foreground and background saved as 'foreground.jpg' and 'background.jpg'.\")\n",
    "\n",
    "# Main function\n",
    "def main(image_path):\n",
    "    # Load SAM model\n",
    "    predictor = load_sam_model()\n",
    "\n",
    "    # Get SAM segmentation mask\n",
    "    image, mask = get_sam_mask(predictor, image_path)\n",
    "\n",
    "    # Segment foreground and background\n",
    "    fg, bg = segment_foreground_background(image, mask)\n",
    "\n",
    "    # Save results\n",
    "    save_results(fg, bg)\n",
    "\n",
    "# Run segmentation on input image\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"dog.jpeg\"  # Change to your image path\n",
    "    main(image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
