{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import exposure\n",
    "\n",
    "# Read the target and reference images\n",
    "target = cv2.imread('input3.jpg')\n",
    "reference = cv2.imread('reference3.jpeg')\n",
    "\n",
    "# Resize reference to match target size\n",
    "reference = cv2.resize(reference, (target.shape[1], target.shape[0]))\n",
    "\n",
    "# Convert target to grayscale\n",
    "target_gray = cv2.cvtColor(target, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert reference and target to Lab color space\n",
    "target_lab = cv2.cvtColor(target, cv2.COLOR_BGR2Lab)\n",
    "reference_lab = cv2.cvtColor(reference, cv2.COLOR_BGR2Lab)\n",
    "\n",
    "# Split Lab channels of target and reference\n",
    "l_tar, a_tar, b_tar = cv2.split(target_lab)\n",
    "l_ref, a_ref, b_ref = cv2.split(reference_lab)\n",
    "\n",
    "# Histogram matching for a and b channels\n",
    "a_matched = exposure.match_histograms(a_tar, a_ref, channel_axis=None)\n",
    "b_matched = exposure.match_histograms(b_tar, b_ref, channel_axis=None)\n",
    "\n",
    "# Merge the grayscale L with matched a and b channels\n",
    "colored_lab = cv2.merge((target_gray, a_matched.astype(np.uint8), b_matched.astype(np.uint8)))\n",
    "\n",
    "# Convert back to BGR color space\n",
    "result = cv2.cvtColor(colored_lab, cv2.COLOR_Lab2BGR)\n",
    "\n",
    "# Save the result\n",
    "cv2.imwrite('output_colored_matched.jpg', result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import wasserstein_distance\n",
    "import ot\n",
    "\n",
    "\n",
    "\n",
    "# Load images\n",
    "target_img = cv2.imread('input1.jpg')\n",
    "reference_img = cv2.imread('reference1.jpeg')\n",
    "target_img_rgb = cv2.cvtColor(target_img, cv2.COLOR_BGR2RGB)\n",
    "reference_img_rgb = cv2.cvtColor(reference_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Load SAM model\n",
    "sam_checkpoint = \"/home/neelraj-reddy/college/6th_sem/computer vision/project/sam_vit_h_4b8939.pth\"  # Replace with correct path if needed\n",
    "model_type = \"vit_h\"\n",
    "# device = \"cuda\"  # Use \"cuda\" if you have a GPU, else \"cpu\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "\n",
    "# Load the SAM model on CPU\n",
    "sam.to('cpu')\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "# Generate masks on CPU\n",
    "target_masks = mask_generator.generate(target_img_rgb)\n",
    "reference_masks = mask_generator.generate(reference_img_rgb)\n",
    "\n",
    "\n",
    "# Extract masked regions and dominant colors using GMM\n",
    "def extract_colors_with_gmm(img, masks, n_components=5):\n",
    "    colors = []\n",
    "    for mask in masks:\n",
    "        binary_mask = mask['segmentation']\n",
    "        region_pixels = img[binary_mask]\n",
    "        \n",
    "        # Fit GMM to extract dominant colors\n",
    "        gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "        gmm.fit(region_pixels)\n",
    "        \n",
    "        # Get mean color of each component\n",
    "        region_colors = gmm.means_\n",
    "        colors.append(region_colors)\n",
    "    \n",
    "    return colors\n",
    "\n",
    "\n",
    "target_colors = extract_colors_with_gmm(target_img_rgb, target_masks)\n",
    "reference_colors = extract_colors_with_gmm(reference_img_rgb, reference_masks)\n",
    "\n",
    "# Apply Optimal Transport (OT) for color mapping\n",
    "def color_transfer_ot(source_colors, target_colors):\n",
    "    transferred_colors = []\n",
    "    \n",
    "    for src_colors, tgt_colors in zip(source_colors, target_colors):\n",
    "        n_source = src_colors.shape[0]\n",
    "        n_target = tgt_colors.shape[0]\n",
    "        \n",
    "        # Cost matrix - Euclidean distance between source and target colors\n",
    "        cost_matrix = np.linalg.norm(src_colors[:, np.newaxis] - tgt_colors, axis=2)\n",
    "        \n",
    "        # Apply Sinkhorn OT\n",
    "        source_weights = np.ones(n_source) / n_source\n",
    "        target_weights = np.ones(n_target) / n_target\n",
    "        ot_plan = ot.sinkhorn(source_weights, target_weights, cost_matrix, reg=0.1)\n",
    "        \n",
    "        # Transfer colors based on optimal mapping\n",
    "        transferred_color = np.dot(ot_plan, tgt_colors)\n",
    "        transferred_colors.append(transferred_color)\n",
    "    \n",
    "    return transferred_colors\n",
    "\n",
    "\n",
    "transferred_colors = color_transfer_ot(reference_colors, target_colors)\n",
    "\n",
    "# Reconstruct the target image with new colors\n",
    "def apply_new_colors(img, masks, transferred_colors):\n",
    "    result_img = img.copy()\n",
    "    for mask, new_colors in zip(masks, transferred_colors):\n",
    "        binary_mask = mask['segmentation']\n",
    "        region_pixels = img[binary_mask]\n",
    "        \n",
    "        # Assign new colors to the region based on transferred colors\n",
    "        for i, pixel in enumerate(region_pixels):\n",
    "            distances = np.linalg.norm(new_colors - pixel, axis=1)\n",
    "            closest_color_idx = np.argmin(distances)\n",
    "            result_img[binary_mask][i] = new_colors[closest_color_idx]\n",
    "    \n",
    "    return result_img\n",
    "\n",
    "\n",
    "result_img = apply_new_colors(target_img_rgb, target_masks, transferred_colors)\n",
    "\n",
    "# Save and display results\n",
    "result_bgr = cv2.cvtColor(result_img, cv2.COLOR_RGB2BGR)\n",
    "cv2.imwrite(\"result_image.jpg\", result_bgr)\n",
    "\n",
    "plt.imshow(result_img)\n",
    "plt.title(\"Color Transferred Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the pre-trained YOLOv8 model for segmentation\n",
    "model = YOLO('yolov8n-seg.pt')  # Use the YOLOv8 segmentation model (small version)\n",
    "\n",
    "def segment_image_yolo(image):\n",
    "    # Perform inference with YOLOv8 on the image\n",
    "    results = model(image)\n",
    "    \n",
    "    # Extract segmentation masks\n",
    "    masks = results.masks  # List of masks for each detected object\n",
    "    return masks\n",
    "\n",
    "def extract_palette(image, mask, num_colors=5):\n",
    "    # Flatten the masked pixels to get colors\n",
    "    pixels = image[mask > 0].reshape(-1, 3)\n",
    "    if len(pixels) == 0:\n",
    "        return np.array([[0, 0, 0]] * num_colors)  # Return dummy palette\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gmm = GaussianMixture(n_components=min(num_colors, len(pixels))).fit(pixels)\n",
    "    return gmm.means_\n",
    "\n",
    "def color_transfer(target, reference, target_mask, reference_mask):\n",
    "    target_palette = extract_palette(target, target_mask)\n",
    "    reference_palette = extract_palette(reference, reference_mask)\n",
    "    \n",
    "    new_target = target.copy()\n",
    "    for i in range(len(target_palette)):\n",
    "        target_color = target_palette[i]\n",
    "        ref_color = reference_palette[i]\n",
    "        diff = ref_color - target_color\n",
    "        new_target[target_mask == i] = np.clip(new_target[target_mask == i] + diff, 0, 255)\n",
    "    return new_target\n",
    "\n",
    "# Load target and reference images\n",
    "target_img = cv2.imread('input.jpg')\n",
    "reference_img = cv2.imread('reference.jpeg')\n",
    "\n",
    "# Segment the images using YOLOv8\n",
    "target_masks = segment_image_yolo(target_img)\n",
    "reference_masks = segment_image_yolo(reference_img)\n",
    "\n",
    "# You can access the masks for each object in the 'masks' list and choose the one you need\n",
    "# Here, we assume the masks are binary and we're processing the first object detected in each image\n",
    "\n",
    "# Assuming a simple case where we are working with the first object mask\n",
    "target_mask = target_masks[0].cpu().numpy()  # Extract the first object mask\n",
    "reference_mask = reference_masks[0].cpu().numpy()  # Extract the first object mask\n",
    "\n",
    "# Perform color transfer based on the masks\n",
    "result_img = color_transfer(target_img, reference_img, target_mask, reference_mask)\n",
    "\n",
    "# Save the resulting image\n",
    "cv2.imwrite('result.jpg', result_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11n-seg.pt\")  # load an official model\n",
    "model = YOLO(\"/home/neelraj-reddy/college/6th_sem/computer vision/project/trying out/yolo11l-seg.pt\")  # load a custom model\n",
    "\n",
    "# Predict with the model\n",
    "results = model(\"/home/neelraj-reddy/college/6th_sem/computer vision/project/trying out/input1.jpg\")  # predict on an image\n",
    "\n",
    "# Access the results\n",
    "for result in results:\n",
    "    xy = result.masks.xy  # mask in polygon format\n",
    "    xyn = result.masks.xyn  # normalized\n",
    "    masks = result.masks.data  # mask in matrix format (num_objects x H x W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load images\n",
    "def load_image(image_path, max_size=400):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    if max(image.size) > max_size:\n",
    "        scale = max_size / float(max(image.size))\n",
    "        new_size = tuple([int(dim * scale) for dim in image.size])\n",
    "        image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.unsqueeze(0)),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(image).to(device)\n",
    "\n",
    "# Convert tensor to image for visualization\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    image = image.permute(1, 2, 0)\n",
    "    image = image.numpy()\n",
    "    image = image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    image = np.clip(image, 0, 1)\n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "# Define a function to compute the gram matrix (used for style)\n",
    "def gram_matrix(input_tensor):\n",
    "    batch_size, channels, height, width = input_tensor.size()\n",
    "    features = input_tensor.view(batch_size * channels, height * width)\n",
    "    gram = torch.mm(features, features.t())\n",
    "    return gram.div(batch_size * channels * height * width)\n",
    "\n",
    "# Define a function for style transfer\n",
    "def style_transfer(target_image, reference_image, num_steps=300, style_weight=1e6, content_weight=1):\n",
    "    # Load pre-trained VGG-19 model\n",
    "    vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "    # Extract content and style layers\n",
    "    content_layers = ['21']  # After 4th block (Relu)\n",
    "    style_layers = ['0', '5', '10', '19']  # Different layers for style transfer\n",
    "\n",
    "    # Get the content and style features of the images\n",
    "    def get_features(image, layers):\n",
    "        features = []\n",
    "        x = image\n",
    "        for name, layer in vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in layers:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "    target_features = get_features(target_image, content_layers + style_layers)\n",
    "    reference_features = get_features(reference_image, style_layers)\n",
    "\n",
    "    target_content = target_features[0]\n",
    "    target_style = target_features[1:]\n",
    "\n",
    "    reference_style = [gram_matrix(f) for f in reference_features]\n",
    "\n",
    "    # Optimize the target image\n",
    "    target_image_opt = target_image.clone().requires_grad_(True)\n",
    "    optimizer = optim.LBFGS([target_image_opt])\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        target_features = get_features(target_image_opt, content_layers + style_layers)\n",
    "\n",
    "        target_content_loss = F.mse_loss(target_features[0], target_content)\n",
    "        target_style_loss = 0\n",
    "        for tf, rs in zip(target_features[1:], reference_style):\n",
    "            target_style_loss += F.mse_loss(gram_matrix(tf), rs)\n",
    "\n",
    "        total_loss = style_weight * target_style_loss + content_weight * target_content_loss\n",
    "        total_loss.backward()\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    # Perform the optimization\n",
    "    for i in range(num_steps):\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Step {i} - Total Loss: {closure():.4f}\")\n",
    "            imshow(target_image_opt, title=f'Step {i}')\n",
    "    \n",
    "    return target_image_opt\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load target and reference images\n",
    "target_image = load_image('input.jpg').to(device)\n",
    "reference_image = load_image('reference.jpeg').to(device)\n",
    "\n",
    "# Perform style transfer\n",
    "output_image = style_transfer(target_image, reference_image)\n",
    "\n",
    "# Show the result\n",
    "imshow(output_image, title='Style Transfer Output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
